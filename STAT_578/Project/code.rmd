---
title: "Text Mining on Yelp Reviews"
author: "Yiming Gao (yimingg2)"
date: "4/4/2018"
output: html_document
---

Some useful references:

- The Statistical Difference between 1-Star and 5-Star Reviews on Yelp: http://minimaxir.com/2014/09/one-star-five-stars/
- Analysis and Visualizations of Yelp Data using R and ggplot2: http://minimaxir.com/2015/12/lets-code-1/
- Sentiment Analysis of Restaurant Reviews: https://rpubs.com/yceron/155272 
- EDA (Filtering): https://rpubs.com/shreyaghelani/234363
- Mexicon restaurants: http://rstudio-pubs-static.s3.amazonaws.com/121639_3364a2eb69b54ed9b85faf1ecf21cd7f.html

The data set contains XX million reviews. It is formatted as by-line JSON: I wrote a pair of Python scripts to convert it to CSV for easy import into R.

The datasets are located here:

- Businesses Data: https://s3.us-east-2.amazonaws.com/578projectyelp/business.csv

Necessary packages:

```{r, message=FALSE, warning=FALSE}
library(stringr)        #This package is used for string manipulation functions
library(dplyr)          #This package is used for data manipulation tasks
library(tidyr)          #This package is used for data manipulation tasks
library(data.table)     #This package is used to access the function fread which is a better/faster way to read large data
library(wordcloud)      #This package is used to generate word clouds
library(tm)             #This is a text mining package used in the process of generating word clouds
# library(RWeka)          # This package is used to generate Bigramws and Trigrams
library(ggplot2)        #This package is used for visualizations (chart/graph plotting functions)
library(ggmap)          #This package is used for map functions 
library(maps)           #This package is used for map functions
library(leaflet)        #This package is used for plotting maps
library(knitr)
library(SnowballC)
library(caret)
library(gmodels)
library(quanteda)
library(tidytext)

# Format accuracy into percentage
percent <- function(x, digits = 2, format = "f", ...) {
  paste0(formatC(100 * x, format = format, digits = digits, ...), "%")
}
```


### Read the data

```{r, message=FALSE, warning=FALSE}
data_business = fread("https://s3.us-east-2.amazonaws.com/578projectyelp/business.csv")

#Dimensions and attribute Names for the Business Data
dim(data_business)
```

### Clean up Reviews Data to filter Reviews for Champaign and Urbana only

```{r}
data_business$categories[1:5]
```

Becasue Categories column in Business Data Frame is a list that can have more than one category for the business, We parsed the categories column and built another data frame that contains information for all "Restaurants". We executed this one time and stored the result in a CSV file.

Now the dataset contains 21892 observations with "Restaurants" in the `categories` column.

```{r}
data_business_restaurants = data_business[data_business$categories %like% "Restaurants", ]

dim(data_business_restaurants)

# Keep relevant columns
data_business_restaurants = data_business_restaurants[, c("business_id", "city", "latitude", "longitude", "name", "categories", "stars", "state", "review_count")]
```

The Reviews Data file was read and filtered for restaurant reviews and further filtered for two cities - Champaign and Urbana. This was done in order to be able to work with a smaller data-set for the scope of this project. Since the original Reviews data is 1.1GB (1569264 rows and 10 columns). The following piece of code was used to get to the reviews dataset for restaurants in Urbana-Champaign from the original reviews dataset.

```{r, message=FALSE, warning=FALSE}
data_review0 = fread("https://s3.us-east-2.amazonaws.com/578projectyelp/review.csv")

# Dimensions and attribute Names for the Business Data
dim(data_review0)

# Keep relevant columns
data_review = data_review0[, c("business_id", "stars", "text", "votes_cool", "votes_funny", "votes_useful")]
```


```{r, message=FALSE, warning=FALSE}
# Business_id of Restaurants in Chambana
chambana_restaurants = data_business_restaurants %>% 
  filter(city == "Champaign" | city == "Urbana") %>%
  select(business_id, name, categories, stars, latitude, longitude, review_count)

# Reviews Data only for Restaurants in Chambana
chambana_reviews = fread("https://s3.us-east-2.amazonaws.com/578projectyelp/chambana_reviews.csv")
```

```{r}
# Final review dataset
dim(chambana_reviews)
names(chambana_reviews)
```


### Preparing the data
For simplicity, only the attributes that are relevant to my project will be listed. It includes the following information:

| Field        |  Description                                                |
|:------------:|:-----------------------------------------------------------:|
| business_id  | The unique identifier for the business                      |
| name         | The full business name                                      |
| categories   | Category this business belongs to

Table: Business Dataset 

# EDA
### A Stacked Bar Chart of 20 Restaurants in Chambana with the highest number of reviews and a segmentation by Rating

```{r}
restaurants_maximum_ratings = chambana_restaurants %>%
  arrange(desc(review_count)) %>% 
  head(20) %>% 
  select(business_id, name, review_count, Average_Rating = stars)

restaurants_maximum_ratings_data = merge(restaurants_maximum_ratings,
                                         chambana_reviews, 
                                         by = "business_id") %>%
  select(business_id, name, review_count, Average_Rating, stars)

ggplot(data = restaurants_maximum_ratings_data, aes(x = reorder(name, review_count), y = review_count, fill = as.factor(stars))) + 
  coord_flip() +
  geom_bar(stat = "identity") + 
  labs(title = "Restaurants in Chambana with the maximum number of reviews",
       y = "Count of Reviews", 
       x = "Restaurant Name", 
       fill = "Rating") + 
  scale_fill_brewer(palette = "YlOrRd")  + 
  theme(axis.text.y = element_text(size = 14))
```

### Where are all the restaurants with more than 4 stars located?

```{r}
top_restaurants = chambana_restaurants[chambana_restaurants$stars >= 4, ]
leaflet(top_restaurants) %>% addTiles() %>%
  addCircleMarkers(lng = ~longitude, lat = ~latitude, radius = 5, fillOpacity = 1 , color = "orange" , popup = ~name) 
```

### What is the average rating of Chinese restaurants?

```{r}

```


# Text Mining
## Wordcloud
### Wordcloud
In this section, we use R packages for text mining of actual texts of Reviews related to Chambana restaurants to find what are the top frequent words used in reviews with different review ratings. We also do similar analysis to find what are frequent words used in reviews related to business ratings and find whether we can find a correlation between these two findings.

The code in below shows the technique we used to find 100 frequent words related to Chambana restaurants with review ratings 4 or 5. (OR 1 &2)

```{r}
data = chambana_reviews
#[chambana_reviews$stars == 4 | chambana_reviews$stars == 5, ]
text = as.vector(data$text)
corpus = VCorpus(VectorSource(text))
corpus = tm_map(corpus, content_transformer(tolower)) # change all letters to lowercase
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeWords, stopwords("SMART"))
corpus = tm_map(corpus, stemDocument) # text stemming

# Build a term-document matrix
dtm = TermDocumentMatrix(corpus)
m = as.matrix(dtm)
v = sort(rowSums(m), decreasing = TRUE)
d = data.frame(word = names(v), freq = v)
head(d, 10)
```

```{r, message=FALSE, warning=FALSE}
# Wordcloud
set.seed(122)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words = 200, random.order = FALSE, rot.per = 0.35, 
          colors = brewer.pal(6, "Dark2"))
```

The above word cloud clearly shows that "place", "good", "food" and "great" are most frequent words in the reviews of restaurants with 4 or 5 stars in Champaign-Urbana.

### Explore frequent terms and their associations
We can have a look at the frequent terms in the term-document matrix as follows. Below shows the words that occur at least 100 times.

```{r}
words.100 = findFreqTerms(dtm, lowfreq = 100)
```

Compute frequency only for selected words

```{r}
dtm.100 = DocumentTermMatrix(corpus, control = list(dictionary = words.100))
```

Top 10 tokens

```{r message=FALSE, warning=FALSE}
term.freq = colSums(as.matrix(dtm.100))
words10 = sort(term.freq, T)[1: 10]
df = data.frame(term = names(words10), freq = words10)
```

```{r}
ggplot(df, aes(x = term, y = freq)) + 
  geom_bar(stat = "identity", fill = "orange", width = 0.5) + 
  xlab("Tokens") + ylab("Count") + 
  coord_flip() + theme_minimal() +
  theme(axis.text = element_text(size = 12))
```

### Relationships between words
Many interesting text analyses are based on the relationships of words. When we examine pairs of two consecutive words, they are often called “bigrams”. Each token now represents a bigram using the following lines of code:

```{r}
bigrams = chambana_reviews %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
# bigrams$bigram
```

After filtering out stop words, what are the most frequent bigrams?

```{r}
bigrams_separated = bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered = bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts = bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

# bigram_counts

bigrams_united = bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```

```{r}
bigram_tf_idf = bigrams_united %>%
  count(bigram)
bigram_tf_idf = bigram_tf_idf %>% filter(n > 100)

ggplot(aes(x = reorder(bigram, n), y = n), data = bigram_tf_idf) + 
  geom_bar(stat = 'identity', fill = "orange", width = 0.4) + 
  ggtitle("The Most Common Bigrams in Reviews of Chambana Restaurants") + 
  coord_flip() + theme_minimal() +
  theme(axis.text = element_text(size = 11))

```

We notice the most frequent bigrams in the reviews except for Champaign-Urbana are Chinese Restaurant, black dog, Mexican food and deep dish.

### Sentiment
`tidytext` package contains several sentiment lexicons, I'm using "bing" for the following tasks. (https://medium.com/@actsusanli/text-mining-is-fun-with-r-35e537b12002, https://github.com/susanli2016/Data-Analysis-with-R/blob/459a9cecfcad5ccf50fe8c02ae701157a62b01b5/Charles_Dickens.Rmd)

```{r message=FALSE, warning=FALSE}
bing_word_counts = d %>%
  inner_join(get_sentiments("bing"), by = "word") 
  # count(word, sentiment) %>%
  # ungroup()
bing_word_counts
```

Here I got the sentiment categories of review words.

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10, wt = freq) %>%
  ungroup() %>%
  mutate(word = reorder(word, freq)) %>%
  ggplot(aes(word, freq, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Words Contribute to sentiment",
       x = NULL) +
  coord_flip() +
  theme_minimal() +
  theme(axis.text = element_text(size = 12))
```

The word "bad" is the most frequent negative word here, which is reasonable. "good", "great" and "love" are three most frequent positive words in the corpus.

Using a word cloud is usually a good idea to identify trends and patterns that would otherwise be unclear or difficult to see in a tabular format. In particular, it compares most frequently used positive and negative words.

```{r message=FALSE, warning=FALSE}
library(reshape2)
bing_word_counts %>%
  acast(word ~ sentiment, value.var = "freq", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100, title.size = 2)
```

### Stars vs sentiment
Apply **NRC sentiment dictionary** to the reviews.

```{r}
library(syuzhet)
star_sentiment = cbind(stars = chambana_reviews$stars, get_nrc_sentiment(chambana_reviews$text))
```

Create a data frame to combine the star of each review with the sentiment score, then extract positive and negative scores for visualization.

```{r}
star_sentiment$negative = -star_sentiment$negative
pos_neg = star_sentiment %>% 
  select(stars, positive, negative) %>% 
  melt(id = "stars") %>%
  group_by(stars) 

names(pos_neg) = c("stars", "sentiment", "value")
```

```{r}
library(ggthemes)
ggplot(data = pos_neg, aes(x = stars, y = value, fill = sentiment)) +
        geom_bar(stat = 'identity', position = position_dodge()) + theme_minimal() +
        ylab("Sentiment") + 
        ggtitle("Positive and Negative Sentiment in Reviews") +
  scale_color_manual(values = c("#00BFC4", "#F8766D")) +
  scale_fill_manual(values = c("#00BFC4", "#F8766D")) +
  theme(axis.text = element_text(size = 16))
```

It seems that reviews with more stars tend to have higher positive and lower negative scores, which is reasonable to us. Due to the computational limitation, I only tried a small part of reviews for visualization.


# Model
## Data Preparation
### Tokenization
Now , we want to re-process all of the reviews and create a *Corpus* object from the text column.

For tokenization, converting review texts to tokens, in this case, words, we use function `DocumentTermMatrix`. The result will be a matrix whose rows are reviews and columns are words that are used in documents. Each value in the matrix represents how many times word in column is been repeated in the reivew of the row. Because not all of the words are used in all reviews, **the matrix is very sparse**. We also want to focus on words that are used in at least 3% of all reviews. We added star rating of each review to the matrix and assigned “good” to 4 and 5 ratings and bad to 0-3 ratings in below, we show results of matrix, dtm.

```{r}
# helper functions
remove1 = function(x) gsub("...", "", x)
remove2 = function(x) gsub("\"\"\"\"", "", x)

data = chambana_reviews
data$target = ifelse(data$stars >= 4, "good", "bad")
text = as.vector(data$text)
corpus = VCorpus(VectorSource(text))
corpus = tm_map(corpus, content_transformer(remove1))
corpus = tm_map(corpus, content_transformer(remove2))
corpus = tm_map(corpus, content_transformer(tolower)) # change all letters to lowercase
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeWords, stopwords("SMART"))
corpus = tm_map(corpus, stemDocument) # text stemming
```

```{r}
dtm = DocumentTermMatrix(corpus)
sparse = removeSparseTerms(dtm, 0.97)
words = as.data.frame(as.matrix(sparse))
```

### N-gram Dictionary
While the word analysis performed in this document is helpful for initial exploration, the data analyst will need to construct a dictionary of bigrams, trigrams, and four-grams, collectively called n-grams. Bigrams are two word phrases, trigrams are three word phrases, and four-grams are four word phrases. Here is an example of trigrams from the randomly sampled twitter corpus. Recall that stop words had been removed so the phrases may look choppy. In the final dictionary, stop phrases and words of any length will be maintained.

```{r}
# Ngram tokenizer
require(quanteda)
tokenize(toLower(chambana_reviews$text[1]), removePunct = TRUE, ngrams = 2)
```

```{r}
# tokenize into tri-grams
trigram.dtm = TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))

# put into data frame
freq.trigram = data.frame(word = trigram.twitterTdm$dimnames$Terms, frequency = trigram.twitterTdm$v)
# reorder by descending frequency
freq.trigram.twitter <- plyr::arrange(freq.trigram.twitter, -frequency)
```

### Training-test Split
Now our data is ready for analysis and prediction. We build two datasets from Chambana data frame. 75% of data will be in training dataset and 25% in testing. To show that data in training and test sets are similarly distributed, we are printing the number of rows in each data set per review stars of good (4/5) and bad(1/2/3).

```{r}
words$target = data$target

# Training-test split
set.seed(122)
inTrain = createDataPartition (y = words$target, p = 0.75, list = FALSE)
trn = words[inTrain, ]
trn = trn[, -c(1, 2)]
tst = words[-inTrain, ]
tst = tst[, -c(1, 2)]
prop.table(table(trn$target))
prop.table(table(tst$target))
```

## Models
Now, we run the model with `caret` package. First we tried the simpliest GLM.

### GLM
```{r message=FALSE, warning=FALSE}
glmFit = train(target ~., data = trn, method = "glm")
preds = predict(glmFit, newdata = tst)
# confusionMatrix(preds, tst$target)
# print(glmFit)

CrossTable(preds, tst$target,
           dnn = c("Predicted", "Actual"),
           prop.t = FALSE, prop.c = FALSE, prop.chisq = FALSE)
```

As the result shown above, from the total of 2080 reviews in test set, GLM model correctly predicted `r sum(preds == tst$target)` ratings, resulting in an accuracy of `r percent(confusionMatrix(preds, tst$target)$overall["Accuracy"])`.

### Naive Bayes

```{r message=FALSE, warning=FALSE}
nb_Fit = train(target ~., data = trn, method = "nb")

preds = predict(nb_Fit, newdata = tst)
# confusionMatrix(preds, tst$target)
# print(glmFit)

CrossTable(preds, tst$target,
           dnn = c("Predicted", "Actual"),
           prop.t = FALSE, prop.c = FALSE, prop.chisq = FALSE)
```

As the result shown above, from the total of 2080 reviews in test set, Naive Bayes model correctly predicted `r sum(preds == tst$target)` ratings, resulting in an accuracy of `r percent(confusionMatrix(preds, tst$target)$overall["Accuracy"])`.
